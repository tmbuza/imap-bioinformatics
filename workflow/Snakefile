from snakemake.utils import min_version

min_version("6.10.0")

# Configuration file containing all user-specified settings
configfile: "config/config.yml"

report: "report/workflow.rst"

import os
import csv
import pandas as pd

# METADATA=pd.read_csv('resources/metadata/metadata.csv').loc[0:3]
# ACCESSIONS=METADATA['run'].tolist() # Specify the column containing the accession, in this demo is Run

# Master rule for controlling workflow.
rule all:
	input:
		"mothur_process/final.shared",
		"mothur_process/final.lefse",
		"mothur_process/final.biom",
		"mothur_process/final.taxonomy",
	
		
		"mothur_process/error_analysis/errorinput.fasta",
		"mothur_process/error_analysis/errorinput.count_table",
		"mothur_process/error_analysis/errorinput.pick.error.summary",

		"mothur_process/intermediate/test.trim.contigs.good.unique.summary",
		
		"images/sra_config_cache.png",
		"images/imap_part02.svg",
		"images/imap_part03.svg",
		"images/imap_part04.svg",
		"images/imap_part05.svg",

		# Create group shared files
		"mothur_process/sample.final.shared",
		"mothur_process/mock.final.shared",
		"mothur_process/control.final.shared",

		# # Create subsample shared file
		# "mothur_process/sample.final.0.03.subsample.shared",
		
		# Alpha and Beta diversity analysis
		"mothur_process/sample.final.count.summary",
		"mothur_process/sample.final.groups.summary",
		"mothur_process/sample.final.groups.rarefaction",
		"mothur_process/sample.final.sharedsobs.0.03.lt.dist",
		"mothur_process/sample.final.thetayc.0.03.lt.dist",
		"mothur_process/sample.final.braycurtis.0.03.lt.dist",
		"mothur_process/sample.final.braycurtis.0.03.lt.tre",
		"mothur_process/sample.final.braycurtis.0.03.lt.pcoa.axes",
		"mothur_process/sample.final.braycurtis.0.03.lt.pcoa.loadings",
		"mothur_process/sample.final.braycurtis.0.03.lt.nmds.iters",
		"mothur_process/sample.final.braycurtis.0.03.lt.nmds.stress",
		"mothur_process/sample.final.braycurtis.0.03.lt.nmds.axes",

		# Error analysis
		"mothur_process/error_analysis/errorinput.fasta",
		"mothur_process/error_analysis/errorinput.count_table",
		"mothur_process/error_analysis/errorinput.pick.error.summary",
		
        # "data/metadata/mothur_mapping_file.tsv",
        # "data/metadata/mothur_metadata_file.tsv",
        # "data/metadata/mothur_design_file.tsv",
        # "resources/metadata/qiime2_manifest_file.tsv",
        # "resources/metadata/qiime2_metadata_file.tsv",

		# "qiime2_process/q2-sample-metadata.qzv",
		# "qiime2_process/demux.qzv",
		# "qiime2_process/rep-seqs.qza",
		# "qiime2_process/feature-table.qza",
		# "qiime2_process/stats.qza",

		"qiime2_process/rep-seqs.qzv",
		"qiime2_process/feature-table.qzv",
		"qiime2_process/stats.qzv",

		# "qiime2_process/feature-table-dn-99.qza",
		# "qiime2_process/rep-seqs-dn-99.qza",
		"qiime2_process/rep-seqs-dn-99.qzv",
		
		"index.html",
		
		# expand("mothur_process/{dataset}.files", dataset=config["dataset"]),

		# "data/references/silva.v4.align",
		# "data/references/trainset16_022016.pds.fasta",
		# "data/references/trainset16_022016.pds.tax",
		# "data/references/zymo.mock.16S.v4.fasta",
	


rule import_mothur_mapping_files:
	output:
		"resources/metadata/mothur_mapping_file.tsv",
		"resources/metadata/mothur_metadata_file.tsv",
		"resources/metadata/mothur_design_file.tsv",
	shell:
		"bash workflow/scripts/import_mothur_metadata.sh"


# Making mothur-based sample mapping file.
rule auto_mothur_mapping_files:
	input:
		script="workflow/scripts/makeFile.sh",
	output:
		files=expand("mothur_process/{dataset}.files", dataset=config["dataset"]),
	conda:
		"envs/mothur.yml"
	shell:
		"bash {input.script}"


# Downloading and formatting SILVA and RDP reference databases. The v4 region is extracted from 
# SILVA database for use as reference alignment.
rule get_mothur_references:
	output:
		silvaV4="data/references/silva.v4.align",
		rdpFasta="data/references/trainset16_022016.pds.fasta",
		rdpTax="data/references/trainset16_022016.pds.tax"
	conda:
		"envs/mothur.yml"
	shell:
		"bash workflow/scripts/mothurReferences.sh"


# Downloading the Zymo mock sequence files and extracting v4 region for error estimation.
rule get_mothur_zymo_mock:
	input:
		script="workflow/scripts/mothurMock.sh",
		silvaV4="data/references/silva.v4.align",
	output:
		mockV4="data/references/zymo.mock.16S.v4.fasta"
	conda:
		"envs/mothur.yml"
	shell:
		"bash {input.script}"

# Generating master OTU shared file.
rule mothur_process_sequences:
	input:
		script="workflow/scripts/mothur_process_seqs.sh",
		files=expand("mothur_process/{dataset}.files", dataset=config["dataset"]),
		silvaV4="data/references/silva.v4.align",
		rdpFasta="data/references/trainset16_022016.pds.fasta",
		rdpTax="data/references/trainset16_022016.pds.tax",
		metadata=rules.import_mothur_mapping_files.output
	output:
		contigreport="mothur_process/test.contigs_report",
		shared="mothur_process/final.shared",
		taxonomy="mothur_process/final.taxonomy",
		lefse="mothur_process/final.lefse",
		biom="mothur_process/final.biom",
		errorfasta="mothur_process/error_analysis/errorinput.fasta",
		errorcount="mothur_process/error_analysis/errorinput.count_table",
	conda:
		"envs/mothur.yml"
	shell:
		"bash {input.script}"


# Calculate estimated sequencing error rate based on mock sequences.
rule mothur_calculate_errorrate:
	input:
		script="workflow/scripts/mothurError.sh",
		errorfasta=rules.mothur_process_sequences.output.errorfasta,
		errorcount=rules.mothur_process_sequences.output.errorcount,
		mockV4=rules.get_mothur_zymo_mock.output.mockV4
	output:
		summary="mothur_process/error_analysis/errorinput.pick.error.summary"
	params:
		mockGroups='-'.join(config["mothurMock"]) # Concatenates all mock group names with hyphens
	conda:
		"envs/mothur.yml"
	shell:
		"bash {input.script} {input.errorfasta} {input.errorcount} {input.mockV4} {params.mockGroups}"


rule remove_intermedeate_files:
	input:
		script="workflow/scripts/clean_intermediate.sh",	
		contigreport=rules.mothur_process_sequences.output.contigreport,
	output:
		"mothur_process/intermediate/test.trim.contigs.good.unique.summary"
	shell:
		"bash {input.script}"


# Splitting master shared file into individual shared file for: i) samples, ii) controls, and iii) mocks.
# This is used for optimal subsampling during downstream steps.
rule mothur_split_group_shared:
	input:
		script="workflow/scripts/mothurSplitShared.sh",
		shared="mothur_process/final.shared"
	output:
		shared=expand("mothur_process/{group}.final.shared", group = config["mothurGroups"])
	params:
		mockGroups='-'.join(config["mothurMock"]), # Concatenates all mock group names with hyphens
		controlGroups='-'.join(config["mothurControl"]) # Concatenates all control group names with hyphens
	conda:
		"envs/mothur.yml"
	shell:
		"bash {input.script} {params.mockGroups} {params.controlGroups}"



# ##################################################################
# #
# # Diversity Metrics 
# #
# #################################################################

rule alpha_beta_diversity:
	input:
		script="workflow/scripts/mothurAlphaBeta.sh",
		# shared="mothur_process/sample.final.shared",
		shared=expand("mothur_process/{group}.final.shared", group = "sample"),
	output:
		subsample="mothur_process/sample.final.0.03.subsample.shared",
		rarefy="mothur_process/sample.final.groups.rarefaction",
	conda:
		"envs/mothur.yml"
	shell:
		"bash {input.script} {input.shared}"



# ##################################################################
#
# QIIME2 pipeline
#
# #################################################################

rule download_qiime2_resources:
	output:
		metadata="qiime2_tutorial/sample_metadata.tsv",
	shell:
		"bash workflow/scripts/download_qiime2_resources.sh"
# #################################################################

rule import_qiime2_mapping_files:
	output:
		manifest="resources/metadata/qiime2_manifest_file.tsv",
		metadata="resources/metadata/qiime2_metadata_file.tsv",
	shell:
		"bash workflow/scripts/import_qiime2_metadata.sh"


rule qiime2_validate_metadata:
	input:
		manifest=rules.import_qiime2_mapping_files.output.manifest
	output:
		"qiime2_process/q2-sample-metadata.qzv"		
	conda:
		"envs/qiime220232.yml"
	shell:
		"bash workflow/scripts/qiime2_validate_metadata.sh"


rule qiime2_import_paired_fastq:
	input:
		manifest=rules.import_qiime2_mapping_files.output.manifest
	output:
		"qiime2_process/demux.qza"
	conda:
		"envs/qiime220232.yml"
	shell:
		"bash workflow/scripts/qiime2_import_paired_fastq.sh"


rule qiime2_summarize_fastq:
	input:
		manifest=rules.qiime2_import_paired_fastq.output
	output:
		"qiime2_process/demux.qzv"
	conda:
		"envs/qiime220232.yml"
	shell:
		"bash workflow/scripts/qiime2_import_paired_fastq.sh"


rule qiime2_read_qc:
	input:
		manifest=rules.qiime2_import_paired_fastq.output
	output:
		repseq="qiime2_process/rep-seqs.qza",
		features="qiime2_process/feature-table.qza",
		stats="qiime2_process/stats.qza",
	conda:
		"envs/qiime220232.yml"
	shell:
		"bash workflow/scripts/qiime2_read_qc.sh"


rule qiime2_create_n_view_qc_qvz:
	input:
		metadata=rules.qiime2_validate_metadata.output,
		qcresults=rules.qiime2_read_qc.output
	output:
		repseqviz="qiime2_process/rep-seqs.qzv",
		featuresviz="qiime2_process/feature-table.qzv",
		statsviz="qiime2_process/stats.qzv",
	conda:
		"envs/qiime220232.yml"
	shell:
		"bash workflow/scripts/qiime2_create_qc_qvz_data.sh"


rule qiime2_denovo_cluster:
	input:
  		"qiime2_process/feature-table.qza",
		"qiime2_process/rep-seqs.qza",
	output:
		"qiime2_process/feature-table-dn-99.qza",
		"qiime2_process/rep-seqs-dn-99.qza",
		"qiime2_process/rep-seqs-dn-99.qzv",
	conda:
		"envs/qiime220232.yml"
	shell:
		"bash workflow/scripts/qiime2_denovo_cluster.sh"


# rule qiime2_closedref_cluster:
# 	input:
#   		"qiime2_process/feature-table.qza",
# 		"qiime2_process/rep-seqs.qza",
# 	output:
# 		"qiime2_process/feature-table-dn-99.qza",
# 		"qiime2_process/rep-seqs-dn-99.qza",
# 		"qiime2_process/rep-seqs-dn-99.qzv",
# 	conda:
# 		"envs/qiime220232.yml"
# 	shell:
# 		"bash workflow/scripts/qiime2_closedref_cluster.sh"


# Get dot rule graphs
rule dot_rules_graph:
	output:
		"dags/rulegraph.svg",
	shell:
		"bash workflow/scripts/rules_dag.sh"


# Get project tree
rule project_tree:
    output:
        tree="results/project_tree.txt",
    shell:
        """
        bash workflow/scripts/tree.sh
        """

# Get smk static report
rule static_snakemake_report:
    output:
        smkhtml="report.html",
        html2png="images/smkreport/screenshot.png",
    shell:
        """
        bash workflow/scripts/smk_html_report.sh
        """

# Images to include in report
rule get_files4smk_report:
	output:
		sracache=report("images/sra_config_cache.png", caption="report/sracache.rst", category="SRA cache"),
		part2=report("images/imap_part02.svg", caption="report/srametadata.rst", category="SRA Metadata DAG"),
		part3=report("images/imap_part03.svg", caption="report/srareads.rst", category="Read Download DAG"),
		part4=report("images/imap_part04.svg", caption="report/readqc.rst", category="Read QC DAG"),
		part5=report("images/imap_part05.svg", caption="report/rulegraph.rst", category="Mothur & QIIME2 DAG"),
	shell:
		"bash workflow/scripts/files4smk_report.sh"


# User styled report for GHPages
rule deploy_to_github_pages:
    output:
        doc="index.html",
    shell:
        """
        R -e "library(rmarkdown); render('index.Rmd')"
        """
